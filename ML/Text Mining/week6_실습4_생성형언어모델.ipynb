{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "daad8471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   0  1\n",
      "0  So there is no way for me to plug it in here i...  0\n",
      "1                        Good case, Excellent value.  1\n",
      "2                             Great for the jawbone.  1\n",
      "3  Tied to charger for conversations lasting more...  0\n",
      "4                                  The mic is great.  1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1000, 2)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# - 데이터 가져 오기\n",
    "\n",
    "import keras\n",
    "import pandas as pd\n",
    "df = pd.read_csv('amazon_cells_labelled.txt', sep='\\t', header=None)\n",
    "print(df.head())\n",
    "df.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7940d6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 일단은 단어 단위로 토큰화를 해준다. \n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tok = Tokenizer()\n",
    "\n",
    "# 이제 df[0] 즉 0번열 실제로 토큰화 및 텍스트를 단어 번호의 리스트로 변환\n",
    "\n",
    "tok.fit_on_texts(df[0])\n",
    "seq = tok.texts_to_sequences(df[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "34fb79d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'So there is no way for me to plug it in here in the US unless I go by a converter.'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 아마존 리뷰 데이터 활용해서 문장 생성 모델 실습\n",
    "# 아마존 리뷰 데이터에서 첫번째 글을 보면 문장의 시작에는 'So'가 나오고, '\n",
    "# So' 다음에는 'there'이 나오며, 'So there' 다음에는 'is'가 나온다. 이러한 관계를 순환신경망에 학습시키면 된다.\n",
    "\n",
    "df.iloc[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "75f5e5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트의 시작과 끝을 나타내는 단어를 사전에 추가\n",
    "\n",
    "tok.word_index['<START>'] = start = len(tok.word_index) + 1\n",
    "tok.index_word[start] = '<START>'\n",
    "\n",
    "tok.word_index['<END>'] = end = len(tok.word_index) + 1\n",
    "tok.index_word[end] = '<END>'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f11b89f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#모든 텍스트의 앞과 뒤에 시작과 끝 표시를 붙여 prev_seq를 만들고, 끝 표시만 붙은 next_seq를 만든다. \n",
    "#이렇게 하면 prev_seq와 next_seq는 한 단어씩 어긋나게 된다. 순환신경망에 prev_seq를 입력으로, next_seq를 출력으로 넣어줄 것이다.\n",
    "prev_seq = []\n",
    "next_seq = []\n",
    "for s in seq:\n",
    "    prev_seq.append([start] + s + [end])\n",
    "    next_seq.append(s + [end])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "49c7eb7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1881   33  117    5   53  214   11   47    8  155    4   19  337   19\n",
      "    1  546  416    2  241  190    6  812 1881    0    0    0    0    0\n",
      "    0    0    0    0]\n",
      "[  33  117    5   53  214   11   47    8  155    4   19  337   19    1\n",
      "  546  416    2  241  190    6  812 1881    0    0    0    0    0    0\n",
      "    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "# 패딩을 위해 텍스트의 최대 길이를 구한다.\n",
    "MAXLEN = max(len(s) for s in prev_seq)\n",
    "\n",
    "\n",
    "# 패딩을 하는데 이전과 달리 뒤에 0을 넣어 채워준다.\n",
    "#from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "prev_pad = pad_sequences(prev_seq, MAXLEN, padding='post')\n",
    "next_pad = pad_sequences(next_seq, MAXLEN, padding='post')\n",
    "\n",
    "print(prev_pad[0])\n",
    "\n",
    "print(next_pad[0])\n",
    "\n",
    "# padding: 문자열, 'pre' 혹은 'post': 각 시퀀스의 처음 혹은 끝을 패딩합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5c83e2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 분할\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(prev_pad, next_pad, test_size=.2, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c2714682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 32, 8)             15056     \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 32, 16)            1600      \n",
      "                                                                 \n",
      " time_distributed_2 (TimeDis  (None, 32, 1882)         31994     \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 48,650\n",
      "Trainable params: 48,650\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모형 만들기\n",
    "NUM_WORDS = len(tok.index_word) + 1\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, TimeDistributed\n",
    "\n",
    "rnn = Sequential()\n",
    "rnn.add(Embedding(input_dim=NUM_WORDS, output_dim=8, input_length=MAXLEN, mask_zero=True))\n",
    "\n",
    "# return_sequences=True로 모든 입력에 대해 출력을 내놓게 한다.\n",
    "rnn.add(LSTM(16, return_sequences=True))\n",
    "\n",
    "# 엑티베이션 함수도 시그모이드가 아닌 여러 개 확률 출력 softmax사용\n",
    "rnn.add(TimeDistributed(Dense(NUM_WORDS, activation='softmax')))\n",
    "\n",
    "# 모델 구성 확인\n",
    "rnn.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a272dfb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 32)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "# 출력의 형태 맞추기\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "58363873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 32, 1)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 출력의 형태가 (None, 32, 1881)인데 데이터는 (800, 32)의 형태이므로 차원이 맞지 않는다. 끝에 1차원을 덧붙여서 형태를 맞춰준다.\n",
    "y_train_dims = numpy.expand_dims(y_train, 2)\n",
    "y_train_dims.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8885a1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\anaconda3\\lib\\site-packages\\keras\\optimizers\\legacy\\adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# 학습 하기\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "rnn.compile(optimizer=Adam(lr=.1), loss='sparse_categorical_crossentropy', metrics=['accuracy'], sample_weight_mode='temporal')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "16361b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "25/25 [==============================] - 3s 37ms/step - loss: 6.1842 - accuracy: 0.0719\n",
      "Epoch 2/30\n",
      "25/25 [==============================] - 1s 40ms/step - loss: 5.2601 - accuracy: 0.1602\n",
      "Epoch 3/30\n",
      "25/25 [==============================] - 1s 55ms/step - loss: 4.9053 - accuracy: 0.1892\n",
      "Epoch 4/30\n",
      "25/25 [==============================] - 1s 37ms/step - loss: 4.6020 - accuracy: 0.2120\n",
      "Epoch 5/30\n",
      "25/25 [==============================] - 1s 38ms/step - loss: 4.3636 - accuracy: 0.2273\n",
      "Epoch 6/30\n",
      "25/25 [==============================] - 1s 39ms/step - loss: 4.1775 - accuracy: 0.2399\n",
      "Epoch 7/30\n",
      "25/25 [==============================] - 1s 37ms/step - loss: 4.0068 - accuracy: 0.2560\n",
      "Epoch 8/30\n",
      "25/25 [==============================] - 1s 37ms/step - loss: 3.8711 - accuracy: 0.2645\n",
      "Epoch 9/30\n",
      "25/25 [==============================] - 1s 38ms/step - loss: 3.7469 - accuracy: 0.2711\n",
      "Epoch 10/30\n",
      "25/25 [==============================] - 1s 36ms/step - loss: 3.6550 - accuracy: 0.2849\n",
      "Epoch 11/30\n",
      "25/25 [==============================] - 1s 36ms/step - loss: 3.5879 - accuracy: 0.2832\n",
      "Epoch 12/30\n",
      "25/25 [==============================] - 1s 38ms/step - loss: 3.5006 - accuracy: 0.2916\n",
      "Epoch 13/30\n",
      "25/25 [==============================] - 1s 37ms/step - loss: 3.4555 - accuracy: 0.2953\n",
      "Epoch 14/30\n",
      "25/25 [==============================] - 1s 37ms/step - loss: 3.4020 - accuracy: 0.3005\n",
      "Epoch 15/30\n",
      "25/25 [==============================] - 1s 37ms/step - loss: 3.3518 - accuracy: 0.3042\n",
      "Epoch 16/30\n",
      "25/25 [==============================] - 1s 39ms/step - loss: 3.3165 - accuracy: 0.3115\n",
      "Epoch 17/30\n",
      "25/25 [==============================] - 1s 38ms/step - loss: 3.2815 - accuracy: 0.3101\n",
      "Epoch 18/30\n",
      "25/25 [==============================] - 1s 37ms/step - loss: 3.2581 - accuracy: 0.3138\n",
      "Epoch 19/30\n",
      "25/25 [==============================] - 1s 37ms/step - loss: 3.2482 - accuracy: 0.3151\n",
      "Epoch 20/30\n",
      "25/25 [==============================] - 1s 43ms/step - loss: 3.2284 - accuracy: 0.3145\n",
      "Epoch 21/30\n",
      "25/25 [==============================] - 1s 40ms/step - loss: 3.2064 - accuracy: 0.3128\n",
      "Epoch 22/30\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 3.1796 - accuracy: 0.3182\n",
      "Epoch 23/30\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 3.1447 - accuracy: 0.3203\n",
      "Epoch 24/30\n",
      "25/25 [==============================] - 1s 37ms/step - loss: 3.1286 - accuracy: 0.3237\n",
      "Epoch 25/30\n",
      "25/25 [==============================] - 1s 37ms/step - loss: 3.1073 - accuracy: 0.3295\n",
      "Epoch 26/30\n",
      "25/25 [==============================] - 1s 36ms/step - loss: 3.0872 - accuracy: 0.3358\n",
      "Epoch 27/30\n",
      "25/25 [==============================] - 1s 37ms/step - loss: 3.0941 - accuracy: 0.3307\n",
      "Epoch 28/30\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 3.0586 - accuracy: 0.3345\n",
      "Epoch 29/30\n",
      "25/25 [==============================] - 1s 35ms/step - loss: 3.0575 - accuracy: 0.3343\n",
      "Epoch 30/30\n",
      "25/25 [==============================] - 1s 36ms/step - loss: 3.0495 - accuracy: 0.3345\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(800, 32)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "rnn.fit(x_train, y_train_dims, epochs=30)\n",
    "\n",
    "y_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "339f4a97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<END>', 'great', 'for', 'the', 'jawbone', '<END>']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 언어 모형을 이용해 문장의 다음 단어를 예측해보자. 예를 들어 첫번째 리뷰의 앞 10단어는 다음과 같다.\n",
    "[tok.index_word[i] for i in prev_seq[2][:10]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0e125769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'it'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이어서 나올 단어는 'it'이다.\n",
    "\n",
    "i = prev_seq[0][10]\n",
    "tok.index_word[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "75036487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<END>', 'so', 'there', 'is', 'no', 'way', 'for', 'me', 'to', 'plug']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이제 RNN으로 예측을 해보자.\n",
    "new_sentence = [prev_seq[0][:10]]\n",
    "\n",
    "# 패딩을 하고\n",
    "new_pad = pad_sequences(new_sentence, MAXLEN, padding='post')\n",
    "\n",
    "[tok.index_word[i] for i in prev_seq[0][:10]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "de71e75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 834ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 32, 1882)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예측을 한다.\n",
    "next_words = rnn.predict(new_pad)\n",
    "\n",
    "\n",
    "# 1개의 텍스트에 대해 32단어 길이로 1881종의 단어에 대한 예측이 나왔다.\n",
    "next_words.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "efe97e3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 가장 확률이 높은 단어는 4번이다.\n",
    "next_words[0, 10].argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ce7bc8fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'it'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.index_word[4] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c5119f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 17ms/step\n",
      "i\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "have\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "had\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "this\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "phone\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "<END>\n"
     ]
    }
   ],
   "source": [
    "# 이를 처음부터 끝까지 반복하게 해서 완전 문장 생성.\n",
    "new_sentence = [[start]]\n",
    "new_pad = pad_sequences(new_sentence, MAXLEN, padding='post')\n",
    "\n",
    "for i in range(MAXLEN - 1):\n",
    "    next_words = rnn.predict(new_pad) # 예측\n",
    "    word = next_words[0, i].argmax()  # 가장 확률이 높은 단어 선정\n",
    "    print(tok.index_word[word])       # 단어 출력\n",
    "    \n",
    "    new_pad[0, i + 1] = word          # 선정 단어를 추가\n",
    "    if word == end:                   # 문장이 끝나면 중단\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ca1c9bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 17ms/step\n",
      "great\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "price\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "<END>\n"
     ]
    }
   ],
   "source": [
    "# 위의 방식으로 하면 매번 같은 문장이 만들어지므로 다양성이 부족하다. 확률이 가장 높은 단어를 선택하는 대신, \n",
    "# 단어를 확률에 따라 무작위로 추출하게 하자.\n",
    "\n",
    "import numpy.random\n",
    "new_sentence = [[start]]\n",
    "new_pad = pad_sequences(new_sentence, MAXLEN, padding='post')\n",
    "\n",
    "for i in range(MAXLEN - 1):\n",
    "    next_words = rnn.predict(new_pad)\n",
    "\n",
    "    # 확률에 따라 단어를 무작위로 추출\n",
    "    word = numpy.random.choice(NUM_WORDS, p=next_words[0, i])\n",
    "\n",
    "    print(tok.index_word[word])\n",
    "    new_pad[0, i + 1] = word\n",
    "    if word == end:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebabbd12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75acbf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
